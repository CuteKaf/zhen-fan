<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Zhen Fan – Research</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div class="main-container">

  <header class="site-header">
    <h1 class="site-title">Zhen Fan</h1>
    <div class="site-subtitle">
      Research – Human–Robot Collaboration, VLA, Sim2Real, Explainable Robotics
    </div>
    <nav class="navbar">
      <a href="index.html">Home</a>
      <a href="research.html" class="active">Research</a>
      <a href="publications.html">Publications</a>
      <a href="projects.html">Projects</a>
      <a href="cv.html">CV</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>

  <section>
    <h2>Research Vision</h2>
    <p>
      My long-term goal is to build <span class="highlight">human-centered, explainable
      human–robot collaboration systems</span> for complex industrial environments.
      I am particularly interested in how language, perception and action can be
      unified into a <span class="highlight">task-semantic representation</span> that
      supports planning, safety, and interaction with human workers.
    </p>
  </section>

  <section>
    <h2>1. Human–Robot Collaboration & XR Interfaces</h2>
    <p>
      I work on XR-based and digital-twin interfaces that make collaboration with
      robots closer to human–human teamwork. In these systems, human operators can
      demonstrate, correct and supervise robot behaviors through intuitive
      interactions, while the system maintains a structured model of tasks and safety
      constraints.
    </p>
    <ul>
      <li>XR interfaces for explaining robot intentions and failure recovery.</li>
      <li>Human-in-the-loop task planning and safety checking.</li>
      <li>Metal additive manufacturing as a testbed for explainable HRC.</li>
    </ul>
  </section>

  <section>
    <h2>2. Vision-Language-Action Models & World Models</h2>
    <p>
      I explore how <span class="highlight">vision-language(-action) models</span>
      and world models can be adapted to industrial workflows. Rather than purely
      end-to-end policies, I aim to combine VLA models with
      <span class="highlight">task semantic graphs</span> and
      <span class="highlight">knowledge graphs</span> so that robot decisions
      remain interpretable and auditable.
    </p>
    <ul>
      <li>Grounding natural language instructions into structured task graphs.</li>
      <li>Using synthetic videos for VLM/VLA adaptation in manufacturing domains.</li>
      <li>Aligning symbolic task structures with continuous control policies.</li>
    </ul>
  </section>

  <section>
    <h2>3. Simulation-to-Real Transfer & Synthetic Data</h2>
    <p>
      High-fidelity digital twins allow us to generate large-scale multimodal data
      for training perception and control models. I build simulation pipelines that
      reduce the cost of data collection by over an order of magnitude while
      preserving real-world performance.
    </p>
    <ul>
      <li>Omniverse-based digital twin for metal additive manufacturing.</li>
      <li>Automatic generation of labeled action videos and sensor data.</li>
      <li>Evaluation of Sim2Real gaps and domain adaptation strategies.</li>
    </ul>
  </section>

  <section>
    <h2>4. Knowledge Graphs & Explainable Robotics</h2>
    <p>
      Technical manuals, standard operating procedures and human expertise are often
      encoded in unstructured documents. I work on extracting this knowledge into
      <span class="highlight">domain knowledge graphs</span> and
      <span class="highlight">task semantic graphs</span> that support
      reasoning, safety checks and natural-language explanations.
    </p>
    <ul>
      <li>Information extraction pipelines for industrial documentation.</li>
      <li>Safety-aware task semantic graphs for HRC.</li>
      <li>Natural language explanations of robot decisions based on graph traversals.</li>
    </ul>
  </section>

</div>
</body>
</html>
