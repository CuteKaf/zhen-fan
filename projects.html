<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Zhen Fan – Projects</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div class="main-container">

  <header class="site-header">
    <h1 class="site-title">Zhen Fan</h1>
    <div class="site-subtitle">
      Selected Research Projects
    </div>
    <nav class="navbar">
      <a href="index.html">Home</a>
      <a href="research.html">Research</a>
      <a href="publications.html">Publications</a>
      <a href="projects.html" class="active">Projects</a>
      <a href="cv.html">CV</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>

  <section>
    <h2>AMSim2Real: Action Recognition for Metal AM</h2>
    <p class="small-muted">
      Simulation-driven human action recognition · Digital twin · Human–Robot Collaboration
    </p>
    <p>
      AMSim2Real is a pipeline that uses a high-fidelity Omniverse digital twin of a
      metal additive manufacturing system to generate synthetic human action data.
      The goal is to recognize real operators’ actions around the metal 3D printer,
      enabling robots and digital assistants to respond appropriately and safely.
    </p>
    <ul>
      <li>Digital twin of the metal AM system with realistic kinematics and workspace.</li>
      <li>Large-scale synthetic videos of human actions for training recognition models.</li>
      <li>Validation on real human operation videos with strong Sim2Real performance.</li>
    </ul>
  </section>

  <section>
    <h2>MetalMind: Knowledge-Graph-Enhanced Digital Twin</h2>
    <p class="small-muted">
      Knowledge graphs · Technical documentation · Explainable HRC
    </p>
    <p>
      MetalMind connects digital twins with domain knowledge extracted from manuals,
      standard operating procedures and human expertise. The system builds a
      <span class="highlight">knowledge graph</span> over tasks, equipment, safety rules
      and human actions, which is then used for planning and explanation.
    </p>
    <ul>
      <li>NER and relation extraction pipelines for industrial documents.</li>
      <li>Task semantic graph for assembly and maintenance procedures.</li>
      <li>Graph-based safety and explainability layer for robot policies.</li>
    </ul>
  </section>

  <section>
    <h2>Synthetic Video Generation for VLM/VLA</h2>
    <p class="small-muted">
      Multimodal data · VLM/VLA adaptation · Manufacturing scenes
    </p>
    <p>
      I use text-to-video models and controlled scene generation to create synthetic
      video datasets for training and adapting vision–language(–action) models
      to manufacturing domains. Prompts cover a wide range of industrial tasks and
      spatial relations, enabling better grounding of spatial reasoning for robots.
    </p>
  </section>

</div>
</body>
</html>
